{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STT-Model&Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdLq8I0Owp8p",
        "outputId": "4748b563-228d-41c9-9756-ddefdef6733f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuxceHHzEn4I"
      },
      "source": [
        "import zipfile\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchaudio\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0VXFBhzal4e"
      },
      "source": [
        "Setup / Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg196w3tp5SJ"
      },
      "source": [
        "assert torch.cuda.is_available(), 'GPU unavailable'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jwCG6Adp6RX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957928b1-d10b-4eba-c9eb-1bc01f89c6f1"
      },
      "source": [
        "print('Number of GPUs available : ', torch.cuda.device_count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available :  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_qatx6gQWdt"
      },
      "source": [
        "MODEL_CHECKPOINT = '/content/drive/MyDrive/Fall2021-CAPSTONE/checkpoint.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icWALG6x2p2U"
      },
      "source": [
        "# zip file handler  \n",
        "zip = zipfile.ZipFile('/content/drive/MyDrive/Fall2021-CAPSTONE/2-DataPreparation/DataSplits_bkp.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRFdag8vY4tE"
      },
      "source": [
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 32\n",
        "\n",
        "# Load size during training\n",
        "load_size = 1280\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 20\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.0006\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary"
      ],
      "metadata": {
        "id": "cyrjM-ayi0hJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3irAHCVCMSW"
      },
      "source": [
        "chars = ' abcdefghijklmnopqrstuvwxyz'\n",
        "vocab = ['<blank>', '<pad>', '<unk>']\n",
        "for ch in chars:\n",
        "  vocab.append(ch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq9iXkwTCoIF",
        "outputId": "f94c2fbd-0473-46dd-d821-97cc6a79ee46"
      },
      "source": [
        "VOCAB_LEN = len(vocab)\n",
        "print('Vocab size: {} (with BLNK, PAD, UNK and SPACE added)'.format(VOCAB_LEN))\n",
        "print('vocab[0]:', vocab[0])\n",
        "print('vocab[1]:', vocab[1])\n",
        "print('vocab[2]:', vocab[2])\n",
        "print('vocab[3]:', vocab[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 30 (with BLNK, PAD, UNK and SPACE added)\n",
            "vocab[0]: <blank>\n",
            "vocab[1]: <pad>\n",
            "vocab[2]: <unk>\n",
            "vocab[3]:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIUFvvRRUVav",
        "outputId": "5810ba91-558a-43b6-a6bd-a5617a98630d"
      },
      "source": [
        "with open('/content/drive/MyDrive/Fall2021-CAPSTONE/max_len.pickle', 'rb') as f2:\n",
        "  MAX_LEN = pickle.load(f2)\n",
        "print('Max target length : ', MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max target length :  105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jmW1OqdBLv5"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjowG85hnmsO"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.spec = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80)\n",
        "\n",
        "  def forward(self, input_X):\n",
        "    X_len = []\n",
        "    AUDIO = []\n",
        "    for X in input_X:\n",
        "      waveform, sample_rate = torchaudio.load(io.BytesIO(X))\n",
        "      audio_tensor = self.spec(waveform).squeeze(0).transpose(1, 0)\n",
        "      # print('spectrogram', audio_tensor.shape)\n",
        "      AUDIO.append(audio_tensor)\n",
        "      X_len.append(audio_tensor.shape[0])\n",
        "    AUDIO = pad_sequence(AUDIO, padding_value=1.)\n",
        "    # print('encoder', AUDIO.shape)\n",
        "    # print(X_len)\n",
        "    return AUDIO, X_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hrOAnUza2vU"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Vmj5pVc9I6"
      },
      "source": [
        "class OverLastDim(nn.Module):\n",
        "    \"\"\"\n",
        "    An n-dimensional tensor of shape (s_1, s_2, ..., s_n) is first collapsed to\n",
        "    a tensor with shape (s_1*s_2*...*s_n-1, s_n). The module is called with\n",
        "    this as input producing (s_1*s_2*...*s_n-1, s_n') --- note that the final\n",
        "    dimension can change. This is expanded to (s_1, s_2, ..., s_n-1, s_n') and\n",
        "    returned.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, module):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self, x):\n",
        "        *dims, input_size = x.size()\n",
        "\n",
        "        reduced_dims = 1\n",
        "        for dim in dims:\n",
        "            reduced_dims *= dim\n",
        "\n",
        "        x = x.view(reduced_dims, -1)\n",
        "        x = self.module(x)\n",
        "        x = x.view(*dims, -1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf9RJBgNu5jx"
      },
      "source": [
        "class RNNWrapper(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, rnn_type=nn.GRU,\n",
        "                 bidirectional=True, batch_norm=True):\n",
        "        \"\"\"Instantiates an RNN without bias parameters. Optionally applies a batch\n",
        "        normalisation layer to the input with the statistics computed over all\n",
        "        time steps. If the RNN is bidirectional, the output from the forward\n",
        "        and backward units is summed before return.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if batch_norm:\n",
        "            self.batch_norm = OverLastDim(nn.BatchNorm1d(input_size))\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn = rnn_type(input_size=input_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            bidirectional=bidirectional,\n",
        "                            bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'batch_norm'):\n",
        "            x = self.batch_norm(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        if self.bidirectional:\n",
        "            # TxNx(H*2) -> TxNxH by sum.\n",
        "            seq_len, batch_size, _ = x.size()\n",
        "            x = x.view(seq_len, batch_size, 2, -1) \\\n",
        "                 .sum(dim=2) \\\n",
        "                 .view(seq_len, batch_size, -1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTo3a98lbAFR"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features=80, n_hidden=MAX_LEN, out_features=VOCAB_LEN, rnn_layers=3, relu_clip=20.):\n",
        "    super().__init__()\n",
        "    \n",
        "    # CONVOLUTIONAL layers\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1,\n",
        "                  out_channels=32,\n",
        "                  kernel_size=5,\n",
        "                  stride=1,\n",
        "                  padding='same'),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.Hardtanh(0, relu_clip, inplace=True),\n",
        "        nn.Conv2d(in_channels=32,\n",
        "                  out_channels=32,\n",
        "                  kernel_size=5,\n",
        "                  stride=1,\n",
        "                  padding='same'),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.Hardtanh(0, relu_clip, inplace=True)\n",
        "    )\n",
        "\n",
        "    # RECURRENT layers\n",
        "    rnn_in_size = 2560\n",
        "    rnns = OrderedDict()\n",
        "    for i in range(rnn_layers):\n",
        "      rnn = RNNWrapper(input_size=rnn_in_size,\n",
        "                        hidden_size=n_hidden,\n",
        "                        rnn_type=nn.GRU,\n",
        "                        bidirectional=True,\n",
        "                        batch_norm=i > 0)\n",
        "      rnns[str(i)] = rnn\n",
        "      rnn_in_size = n_hidden\n",
        "    self.rnns = nn.Sequential(rnns)\n",
        "\n",
        "    # FULLY CONNECTED layers\n",
        "    fully_connected = nn.Sequential(\n",
        "        nn.BatchNorm1d(n_hidden),\n",
        "        nn.Linear(n_hidden, out_features, bias=False)\n",
        "    )\n",
        "    self.fc = OverLastDim(fully_connected)\n",
        "\n",
        "  # for training\n",
        "  def forward(self, X_in, X_len, Y_in, Y_len):\n",
        "    \"\"\"\n",
        "    Perform token prediction and compute loss over training set.\n",
        "    \n",
        "    Inputs:\n",
        "    - X_in: A tensor of shape (seq_len, batch, in_features)\n",
        "      containing a mini-batch of audio sequence features padded to seq_len.\n",
        "    - X_len: A tuple of shape (batch, ) containing the \n",
        "      actual lengths of the audio sequence (each <= seq_len).\n",
        "    - Y_in: A tensor of shape (batch, max_seq_len)\n",
        "      containing a mini-batch of text targets padded to max_seq_len.\n",
        "      Each element in the target sequence is an index in the vocabulary. \n",
        "      And the target index cannot be blank (index=0 in vocab).\n",
        "    - Y_len: A tuple of shape (batch, ) containing the \n",
        "      actual lengths of the targets (each <= max_seq_len).\n",
        "    \n",
        "    Returns:\n",
        "    - loss: A PyTorch scalar containing the CTC loss for the mini-batch.\n",
        "    \"\"\"\n",
        "    # training logic here\n",
        "    # print('before conv', X_in.shape)\n",
        "    X_in = X_in.permute(1, 2, 0)   # TxNxH -> NxHxT\n",
        "    X_in.unsqueeze_(dim=1)      # NxHxT -> Nx1xHxT\n",
        "    X_in = self.conv(X_in)\n",
        "    # print('after conv', X_in.shape)\n",
        "\n",
        "    N, H1, H2, T = X_in.size()\n",
        "    x = X_in.view(N, H1*H2, T)\n",
        "    x = x.permute(2, 0, 1)   # NxHxT -> TxNxH\n",
        "    x = self.rnns(x.contiguous())\n",
        "    # print('after rnns', x.shape)\n",
        "\n",
        "    out = self.fc(x)\n",
        "    logprobs = nn.functional.log_softmax(out, dim=2)\n",
        "    # print('decoder', logprobs.shape)\n",
        "    \n",
        "    # compute CTC loss\n",
        "    ctc_loss = nn.CTCLoss(zero_infinity=True)\n",
        "    loss = ctc_loss(logprobs, Y_in, X_len, Y_len)\n",
        "    return loss \n",
        "\n",
        "  # for inference\n",
        "  def predict(self, X_in):\n",
        "    \"\"\"\n",
        "    Perform token prediction over validation/test set.\n",
        "    \n",
        "    Inputs:\n",
        "    - X_in: A tensor of shape (seq_len, batch, in_features)\n",
        "      containing a mini-batch of audio features padded to seq_len.\n",
        "    \n",
        "    Returns:\n",
        "    - text: A tuple of shape (batch_size, )\n",
        "      containing text output for the given batch.\n",
        "    \"\"\"\n",
        "    # inference logic here\n",
        "    X_in = X_in.permute(1, 2, 0)   # TxNxH -> NxHxT\n",
        "    X_in.unsqueeze_(dim=1)      # NxHxT -> Nx1xHxT\n",
        "    X_in = self.conv(X_in)\n",
        "\n",
        "    N, H1, H2, T = X_in.size()\n",
        "    x = X_in.view(N, H1*H2, T)\n",
        "    x = x.permute(2, 0, 1)   # NxHxT -> TxNxH\n",
        "    x = self.rnns(x.contiguous())\n",
        "    out = self.fc(x)\n",
        "    logprobs = nn.functional.log_softmax(out, dim=2)\n",
        "    _, max_indices = logprobs.float().max(2)\n",
        "    \n",
        "    batch_sentences = []\n",
        "    for i, indices in enumerate(max_indices.t()):\n",
        "        no_dups, prev = [], None\n",
        "        for index in indices:\n",
        "            if prev is None or index != prev:\n",
        "                no_dups.append(index.item())\n",
        "                prev = index\n",
        "\n",
        "        symbols = [vocab[s] for s in no_dups]\n",
        "\n",
        "        no_blanks = [s for s in symbols if (s!=vocab[0] and s!=vocab[1])]\n",
        "        batch_sentences.append(''.join(no_blanks))\n",
        "    return batch_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Np8cSUWa8h6"
      },
      "source": [
        "Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGIhZOJoBKkY"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, listOfFiles):\n",
        "      '''\n",
        "      Takes as input the list of file paths containing X-audio, Y-text data.\n",
        "      Stores the audio data in a member variable X.\n",
        "      Stores the text data in a member variable Y.\n",
        "      '''\n",
        "      # opening the files and storing their contents in lists\n",
        "      audio_list = []\n",
        "      listOfAudioFiles = [x for x in listOfFiles if x.endswith('.wav')]\n",
        "      for audio_file in listOfAudioFiles:\n",
        "        audio_list.append(zip.read(audio_file))\n",
        "      \n",
        "      text_list = []\n",
        "      listOfTxtFiles = [x for x in listOfFiles if x.endswith('.txt')]\n",
        "      for text_file in listOfTxtFiles:\n",
        "        loaded_txt = zip.read(text_file)\n",
        "        text_list.append(loaded_txt)\n",
        "\n",
        "      # store them in member variables\n",
        "      self.X = audio_list\n",
        "      self.Y = text_list\n",
        "    \n",
        "    def __len__(self):\n",
        "      return len(self.Y)\n",
        "   \n",
        "    def __getitem__(self, index):\n",
        "      '''\n",
        "      Returns the X,Y pair present at the specified index of the list.\n",
        "      '''\n",
        "      return self.X[index], self.Y[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xroje26dU3_k"
      },
      "source": [
        "def encode_targets(Y_batch):\n",
        "  text = torch.ones((len(Y_batch), MAX_LEN), dtype=torch.int64)\n",
        "  sent_lengths = []\n",
        "  for sent_idx, sent in enumerate(Y_batch):\n",
        "    sent_lengths.append(len(sent))\n",
        "    symbols = list(sent.lower())\n",
        "    for idx, symbol in enumerate(symbols):\n",
        "      ch = chr(symbol)\n",
        "      if ch in vocab:\n",
        "        text[sent_idx][idx] = vocab.index(ch)\n",
        "      else:\n",
        "        text[sent_idx][idx] = 2\n",
        "  # print('target', text.shape)\n",
        "  return text, tuple(sent_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZNuhu6y6HF7"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO9xfJy1Gk0W"
      },
      "source": [
        "# Note : Just a feature extractor (no trainable params).\n",
        "enc = Encoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEvVEjJw4lQo",
        "outputId": "bf7b7f27-68d8-41dc-b763-da6a9d5c044a"
      },
      "source": [
        "dec = Decoder()\n",
        "dec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Hardtanh(min_val=0, max_val=20.0, inplace=True)\n",
              "    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
              "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): Hardtanh(min_val=0, max_val=20.0, inplace=True)\n",
              "  )\n",
              "  (rnns): Sequential(\n",
              "    (0): RNNWrapper(\n",
              "      (rnn): GRU(2560, 105, bias=False, bidirectional=True)\n",
              "    )\n",
              "    (1): RNNWrapper(\n",
              "      (batch_norm): OverLastDim(\n",
              "        (module): BatchNorm1d(105, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (rnn): GRU(105, 105, bias=False, bidirectional=True)\n",
              "    )\n",
              "    (2): RNNWrapper(\n",
              "      (batch_norm): OverLastDim(\n",
              "        (module): BatchNorm1d(105, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (rnn): GRU(105, 105, bias=False, bidirectional=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): OverLastDim(\n",
              "    (module): Sequential(\n",
              "      (0): BatchNorm1d(105, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Linear(in_features=105, out_features=30, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq6ocV2nP-nU"
      },
      "source": [
        "loss_history = []\n",
        "\n",
        "epoch = 0\n",
        "load_index = 0\n",
        "batch_index = 0\n",
        "loss = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nantvsLUQQFm"
      },
      "source": [
        "checkpoint = None\n",
        "try:\n",
        "  checkpoint = torch.load(MODEL_CHECKPOINT, map_location=torch.device('cpu'))\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjZUSC-4KuUb"
      },
      "source": [
        "if checkpoint is not None and bool(checkpoint):\n",
        "  if 'epoch' in checkpoint:\n",
        "    epoch = checkpoint['epoch']\n",
        "\n",
        "  if 'load_index' in checkpoint:\n",
        "    load_index = checkpoint['load_index']\n",
        "\n",
        "  if 'batch_index' in checkpoint: \n",
        "    batch_index = checkpoint['batch_index']\n",
        "\n",
        "  if 'loss' in checkpoint:\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "  if 'loss_history' in checkpoint and len(checkpoint['loss_history']) > 0:\n",
        "    loss_history = [float(x) for x in checkpoint['loss_history'].split(',')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27K5kPBHKkzZ"
      },
      "source": [
        "if 'model_state_dict' in checkpoint:\n",
        "  dec.load_state_dict(checkpoint['model_state_dict'])\n",
        "dec.cuda()\n",
        "\n",
        "optimizer = optim.Adam(dec.parameters(), lr=lr)\n",
        "if 'optimizer_state_dict' in checkpoint:\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r4RRcr6Z-M8",
        "outputId": "d6c74118-07f2-4f2e-c90b-b0415b4c749b"
      },
      "source": [
        "with open('/content/drive/MyDrive/Fall2021-CAPSTONE/train_list.pickle', 'rb') as f3:\n",
        "  train_list = pickle.load(f3)\n",
        "print('Total training load : ', len(train_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training load :  856210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpRi4iTfsdne",
        "outputId": "20c9ce24-1863-435c-9112-60553b67cb73"
      },
      "source": [
        "print(\"Starting Training Loop...\")\n",
        "dec.train()\n",
        "# For each epoch\n",
        "while epoch < num_epochs:\n",
        "  while load_index < math.ceil(len(train_list)/load_size):\n",
        "    # create a dataset object\n",
        "    dset = CustomDataset(train_list[load_index*load_size:(load_index*load_size)+load_size])\n",
        "    # wrap it around a dataloader\n",
        "    data_loader = DataLoader(dset, batch_size = batch_size, num_workers = workers)\n",
        "\n",
        "    for idx, ( X_batch, Y_batch) in enumerate(data_loader):\n",
        "      if idx < batch_index:\n",
        "        continue\n",
        "      # forward propagation\n",
        "      dec.zero_grad()\n",
        "      Y_in, Y_len = encode_targets(Y_batch)\n",
        "      X_in, X_len = enc(X_batch)\n",
        "      \n",
        "      loss = dec(X_in.cuda(), X_len, Y_in.cuda(), Y_len)\n",
        "      # loss = dec(X_in, X_len, Y_in, Y_len)\n",
        "\n",
        "      # backward propagation\n",
        "      loss.backward()\n",
        "\n",
        "      # parameter updates\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_index += 1\n",
        "    \n",
        "    batch_index = 0\n",
        "    load_index += 1\n",
        "    with torch.no_grad():\n",
        "      print('Train Epoch: {:3} \\t Load: {:3} \\t Loss: {:F}'.format(epoch, load_index, loss.item()))\n",
        "      dec.cpu()\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': dec.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'load_index': load_index,\n",
        "          'batch_index': batch_index,\n",
        "          'loss': loss,\n",
        "          'loss_history': ','.join([str(x) for x in loss_history])\n",
        "          }, MODEL_CHECKPOINT)\n",
        "      dec.cuda()\n",
        "  \n",
        "  load_index = 0\n",
        "  epoch += 1\n",
        "  with torch.no_grad():\n",
        "    loss_history.append(loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEOFlf7cUtlU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}